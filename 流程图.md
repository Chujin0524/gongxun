graph TD
    A[开始] --> B[动作生成]
    B --> C[根据当前状态 s_t]
    C --> D[Actor网络生成动作 a_t]
    D --> E[动作 a_t = [ΔK_att, ΔK_rep, Δλ] + N(0, σ)]
    E --> F[ΔK_att, ΔK_rep: 引力与斥力增益系数的动态调整量]
    E --> G[Δλ: 曲率约束权重系数的调整量]
    E --> H[N(0, σ): 高斯噪声，增强策略探索能力]
    
    B --> I[动态增益更新]
    I --> J[根据动作输出更新势场参数]
    J --> K[K_att(t+1) = K_att(t) + η⋅ΔK_att]
    J --> L[K_rep(t+1) = K_rep(t) + η⋅ΔK_rep]
    J --> M[λ(t+1) = λ(t) + η⋅Δλ]
    M --> N[η: 学习率，控制参数更新步长]
    
    N --> O[结束]